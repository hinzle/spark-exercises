{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# current filepath system leaves plenty to be desired\n",
    "import sys\n",
    "local_path = '/Users/hinzlehome/codeup-data-science/spark-exercises/'\n",
    "sys.path.insert(0, local_path)\n",
    "\n",
    "# imports.py in /utils/\n",
    "from utils.imports import *\n",
    "\n",
    "# plotting magic\n",
    "%matplotlib inline\n",
    "# plotting defaults\n",
    "plt.rc('figure', figsize=(16, 9))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rc('font', size=16)\n",
    "# plt.style.available\n",
    "# ^^^ show available seaborn styles\n",
    "\n",
    "# !!! Warning !!! \n",
    "# *** no more warnings ***\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# custom mods\n",
    "# from utils.tidy import *\n",
    "# from utils.model import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark API Mini Exercises\n",
    "\n",
    "Copy the code below to create a pandas dataframe with 20 rows and 3 columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/19 11:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: double, group: string, abool: boolean]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. What is the difference between `.show` and `.head`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[n: double, group: string, abool: boolean]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(n=-0.712390662050588, group='z', abool=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. View a summary of the data using `.describe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, n: string, group: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "   columns. View the first 5 rows of this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n','abool').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "   columns. View the first 5 rows of this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('group','abool').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use `.select` to create a new dataframe with the `group` column and the\n",
    "   `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "   this dataframe.\n",
    "7. Use `.select` to create a new dataframe with the `group` column and the\n",
    "   `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "   dataframe.\n",
    "\n",
    "2. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    2. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    3. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    4. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    5. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    6. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    7. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    8. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    9. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?\n",
    "\n",
    "3. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    2. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    3. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    4. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    5. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    6. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    7. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    8. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?\n",
    "\n",
    "4. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Import the necessary functions from `pyspark.sql.functions`\n",
    "    3. Find the highest `n` value.\n",
    "    4. Find the lowest `n` value.\n",
    "    5. Find the average `n` value.\n",
    "    6. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    7. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\"\n",
    "\n",
    "5. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    3. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value.\n",
    "\n",
    "6. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    3. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    4. Find the columns where the `group` column is *not* `y`.\n",
    "    5. Find the columns where `n` is positive.\n",
    "    6. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    7. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    8. Find the columns where `abool` is false and `n` is less than 1\n",
    "    9. Find the columns where `abool` is false or `n` is less than 1\n",
    "\n",
    "7. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Sort by the `n` value.\n",
    "    3. Sort by the `group` value, both ascending and descending.\n",
    "    4. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    5. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n",
    "\n",
    "8. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    2. What is the maximum `n` value for each group in the `group` column?\n",
    "    3. What is the minimum `n` value by `abool`?\n",
    "    4. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?\n",
    "\n",
    "9.  Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    3. What happens if you make a SQL syntax error in your query?\n",
    "    4. Write a query that shows all of the columns from your dataframe.\n",
    "    5. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    6. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    7. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
