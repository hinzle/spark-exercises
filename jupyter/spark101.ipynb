{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# current filepath system leaves plenty to be desired\n",
    "import sys\n",
    "local_path = '/Users/hinzlehome/codeup-data-science/spark-exercises/'\n",
    "sys.path.insert(0, local_path)\n",
    "\n",
    "# imports.py in /utils/\n",
    "from utils.imports import *\n",
    "\n",
    "# plotting magic\n",
    "%matplotlib inline\n",
    "# plotting defaults\n",
    "plt.rc('figure', figsize=(16, 9))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rc('font', size=16)\n",
    "# plt.style.available\n",
    "# ^^^ show available seaborn styles\n",
    "\n",
    "# !!! Warning !!! \n",
    "# *** no more warnings ***\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# custom mods\n",
    "# from utils.tidy import *\n",
    "# from utils.model import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pandas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    0.4\n",
       "z    0.3\n",
       "x    0.3\n",
       "Name: group, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/19 11:33:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: double, group: string, abool: boolean]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. What is the difference between `.show` and `.head`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[n: double, group: string, abool: boolean]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(n=-0.712390662050588, group='z', abool=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. View a summary of the data using `.describe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, n: string, group: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "   columns. View the first 5 rows of this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n','abool').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "   columns. View the first 5 rows of this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('group','abool').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use `.select` to create a new dataframe with the `group` column and the\n",
    "   `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "   this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|a_boolean_value|\n",
      "+-----+---------------+\n",
      "|    z|          false|\n",
      "|    x|          false|\n",
      "|    z|          false|\n",
      "+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('group',F.col('abool').alias('a_boolean_value')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `.select` to create a new dataframe with the `group` column and the\n",
    "   `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "   dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|     a_numeric_value|\n",
      "+-----+--------------------+\n",
      "|    z|  -0.712390662050588|\n",
      "|    x|   0.753766378659703|\n",
      "|    z|-0.04450307833805...|\n",
      "|    y| 0.45181233874578974|\n",
      "|    z|  1.3451017084510097|\n",
      "|    y|  0.5323378882945463|\n",
      "+-----+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('group',F.col('n').alias('a_numeric_value')).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Column Manipulation\n",
    "\n",
    "1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Use `.select` to add 4 to the `n` column. Show the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                   n|           (n + 4)|\n",
      "+--------------------+------------------+\n",
      "|  -0.712390662050588|3.2876093379494122|\n",
      "|   0.753766378659703| 4.753766378659703|\n",
      "|-0.04450307833805...|3.9554969216619464|\n",
      "| 0.45181233874578974|  4.45181233874579|\n",
      "|  1.3451017084510097|5.3451017084510095|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n',df.n+4).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Subtract 5 from the `n` column and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                   n|            (n - 5)|\n",
      "+--------------------+-------------------+\n",
      "|  -0.712390662050588| -5.712390662050588|\n",
      "|   0.753766378659703| -4.246233621340297|\n",
      "|-0.04450307833805...| -5.044503078338053|\n",
      "| 0.45181233874578974|  -4.54818766125421|\n",
      "|  1.3451017084510097|-3.6548982915489905|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n',df.n-5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Multiply the `n` column by 2. View the results along with the original number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|             (n * 2)|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -1.424781324101176|\n",
      "|   0.753766378659703|   1.507532757319406|\n",
      "|-0.04450307833805...|-0.08900615667610691|\n",
      "| 0.45181233874578974|  0.9036246774915795|\n",
      "|  1.3451017084510097|  2.6902034169020195|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n',df.n*2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|                  n2|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|   0.712390662050588|\n",
      "|   0.753766378659703|  -0.753766378659703|\n",
      "|-0.04450307833805...|0.044503078338053455|\n",
      "| 0.45181233874578974|-0.45181233874578974|\n",
      "+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n',F.expr('n*-1 AS n2')).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|                  n2|                  n3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|   0.712390662050588|   0.507500455376875|\n",
      "|   0.753766378659703|  -0.753766378659703|  0.5681637535977627|\n",
      "|-0.04450307833805...|0.044503078338053455|0.001980523981562...|\n",
      "| 0.45181233874578974|-0.45181233874578974| 0.20413438944294027|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n',F.expr('n*-1 AS n2'),F.expr('pow(n,2) AS n3')).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    7. What happens when you run the code below?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(group + abool)'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group + df.abool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    8. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(CAST(group AS DOUBLE) + abool)' due to data type mismatch: differing types in '(CAST(group AS DOUBLE) + abool)' (double and boolean).;\n'Project [unresolvedalias((cast(group#199 as double) + abool#200), Some(org.apache.spark.sql.Column$$Lambda$3950/0x0000000801616040@28ce92f))]\n+- LogicalRDD [n#198, group#199, abool#200], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hinzlehome/codeup-data-science/spark-exercises/jupyter/spark_api_mini.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hinzlehome/codeup-data-science/spark-exercises/jupyter/spark_api_mini.ipynb#ch0000041?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mselect(df\u001b[39m.\u001b[39;49mgroup \u001b[39m+\u001b[39;49m df\u001b[39m.\u001b[39;49mabool)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1663'>1664</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols):\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1664'>1665</a>\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1665'>1666</a>\u001b[0m \n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1666'>1667</a>\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1682'>1683</a>\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1683'>1684</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1684'>1685</a>\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1685'>1686</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(group AS DOUBLE) + abool)' due to data type mismatch: differing types in '(CAST(group AS DOUBLE) + abool)' (double and boolean).;\n'Project [unresolvedalias((cast(group#199 as double) + abool#200), Some(org.apache.spark.sql.Column$$Lambda$3950/0x0000000801616040@28ce92f))]\n+- LogicalRDD [n#198, group#199, abool#200], false\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df.select(df.group + df.abool)\n",
    "\n",
    "# this code tries to execute an action, so theres an error cuz type mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    9. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(n + abool)' due to data type mismatch: differing types in '(n + abool)' (double and boolean).;\n'Project [unresolvedalias((n#198 + abool#200), Some(org.apache.spark.sql.Column$$Lambda$3950/0x0000000801616040@28ce92f))]\n+- LogicalRDD [n#198, group#199, abool#200], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hinzlehome/codeup-data-science/spark-exercises/jupyter/spark_api_mini.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hinzlehome/codeup-data-science/spark-exercises/jupyter/spark_api_mini.ipynb#ch0000042?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mselect(df\u001b[39m.\u001b[39;49mn \u001b[39m+\u001b[39;49m df\u001b[39m.\u001b[39;49mabool)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1663'>1664</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols):\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1664'>1665</a>\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1665'>1666</a>\u001b[0m \n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1666'>1667</a>\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1682'>1683</a>\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1683'>1684</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1684'>1685</a>\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1685'>1686</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(n + abool)' due to data type mismatch: differing types in '(n + abool)' (double and boolean).;\n'Project [unresolvedalias((n#198 + abool#200), Some(org.apache.spark.sql.Column$$Lambda$3950/0x0000000801616040@28ce92f))]\n+- LogicalRDD [n#198, group#199, abool#200], false\n"
     ]
    }
   ],
   "source": [
    "# df.select(df.n + df.abool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(n + group)|\n",
      "+-----------+\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n + df.group).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    2. Use `.printSchema` to view the datatypes in your dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- n: double (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- abool: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    3. Use `.dtypes` to view the datatypes in your dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 'double'), ('group', 'string'), ('abool', 'boolean')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    4. What is the difference between the two code samples below?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'CAST(abool AS INT)'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.abool.cast('int')\n",
    "\n",
    "# this returns a column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5)\n",
    "\n",
    "# this returns a column print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    5. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group.cast('int')).show(5)\n",
    "\n",
    "# all nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n.cast('int')).show(5)\n",
    "\n",
    "# all ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "|false|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('string')).show(5)\n",
    "\n",
    "# all strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Import the necessary functions from `pyspark.sql.functions`\n",
    "\n",
    "done from imports\n",
    "\n",
    "imported as 'F'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Find the highest `n` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            max(n)|\n",
      "+------------------+\n",
      "|2.1503829673811126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.max('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Find the lowest `n` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            min(n)|\n",
      "+------------------+\n",
      "|-1.261605945319069|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.min('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Find the average `n` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            avg(n)|\n",
      "+------------------+\n",
      "|0.3664026449885216|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.avg('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   Group|\n",
      "+--------+\n",
      "|Group: z|\n",
      "|Group: x|\n",
      "|Group: z|\n",
      "|Group: y|\n",
      "|Group: z|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat(F.lit('Group: '),df.group).alias('Group')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|Group                   |\n",
      "+------------------------+\n",
      "|z: -0.712390662050588   |\n",
      "|x: 0.753766378659703    |\n",
      "|z: -0.044503078338053455|\n",
      "|y: 0.45181233874578974  |\n",
      "|z: 1.3451017084510097   |\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat(df.group,F.lit(': '),df.n).alias('Group')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\" when `abool` is false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|abool|     It is:|\n",
      "+-----+-----------+\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "|false|It is false|\n",
      "| true| It is true|\n",
      "| true| It is true|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('abool',F.when(df.abool == True, \"It is true\").otherwise(\"It is false\").alias(\"It is:\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                   n|        short title|\n",
      "+--------------------+-------------------+\n",
      "|  -0.712390662050588|                0.0|\n",
      "|   0.753766378659703|  0.753766378659703|\n",
      "|-0.04450307833805...|                0.0|\n",
      "| 0.45181233874578974|0.45181233874578974|\n",
      "|  1.3451017084510097| 1.3451017084510097|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('n', F.when(df.n<0,0).otherwise(df.n).alias(\"short title\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|0.45181233874578974|    y|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "|-1.0453771305385342|    y| true|\n",
      "| -1.261605945319069|    y|false|\n",
      "| 0.5628467852810314|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.group=='y').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.abool==False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Find the columns where the `group` column is *not* `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.group!='y').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Find the columns where `n` is positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|  0.753766378659703|    x|false|\n",
      "|0.45181233874578974|    y|false|\n",
      "| 1.3451017084510097|    z|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "| 1.3501878997225267|    z|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.n>0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Find the columns where `abool` is true and the `group` column is `z`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "| 1.3451017084510097|    z|false|\n",
      "| 1.3501878997225267|    z|false|\n",
      "| 1.4786857374358966|    z| true|\n",
      "|0.12730328020698067|    z|false|\n",
      "+-------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.n>0).filter(df.group=='z').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Find the columns where `abool` is true or the `group` column is `z`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|1.4786857374358966|    z| true|\n",
      "+------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.abool==True).filter(df.group=='z').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Find the columns where `abool` is false and `n` is less than 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.abool==False).where(df.n<1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    9. Find the columns where `abool` is false or `n` is less than 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False)|(df.n<1)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Sort by the `n` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -1.261605945319069|    y|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -0.712390662050588|    z|false|\n",
      "|-0.24332625188556253|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('n').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Sort by the `group` value, both ascending and descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|  1.4786857374358966|    z| true|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|   0.753766378659703|    x|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort(df['group'].desc()).show(5),df.sort(df.group.asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.7889890249515489|    x|false|\n",
      "|-0.02677164998644...|    x| true|\n",
      "| 0.31735092273633597|    x|false|\n",
      "|  0.6062886568962988|    x|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  0.5628467852810314|    y| true|\n",
      "|  0.9137407048596775|    y|false|\n",
      "|  2.1503829673811126|    y| true|\n",
      "|  -0.712390662050588|    z|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  1.4786857374358966|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('group','n').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.7889890249515489|    x|false|\n",
      "|0.31735092273633597|    x|false|\n",
      "| 0.6062886568962988|    x|false|\n",
      "|  0.753766378659703|    x|false|\n",
      "| 0.8612113741693206|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('abool','group','n').show(5)\n",
    "\n",
    "# order matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the maximum `n` value for each group in the `group` column?\n",
    "    3. What is the minimum `n` value by `abool`?\n",
    "    4. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?\n",
    "\n",
    "1.  Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    2. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    3. What happens if you make a SQL syntax error in your query?\n",
    "    4. Write a query that shows all of the columns from your dataframe.\n",
    "    5. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    6. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    7. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
